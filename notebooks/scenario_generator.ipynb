{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Preservation Scenario Generator\n",
    "\n",
    "This notebook generates self-preservation scenarios at scale using variable pools, templates, and LLM filling.\n",
    "\n",
    "**Features:**\n",
    "- Stratified sampling across domains, threat types, and tool capabilities\n",
    "- Hybrid generation: templates for structure, LLM for realistic details\n",
    "- Support for vLLM (local GPU) and OpenRouter (cloud API)\n",
    "- Checkpointing for recovery\n",
    "- Automatic validation\n",
    "\n",
    "**Output:**\n",
    "- Compatible with `prefill_scenario_runner.ipynb`\n",
    "- Organized in batches with manifest files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from datagen.scenario_generator import SelfPreservationScenarioGenerator\n",
    "from datagen.llm_filler import AsyncLLMFiller\n",
    "from datagen.config import GenerationConfig, get_quick_test_config, get_development_config\n",
    "from datagen.variable_pools import DOMAINS, THREAT_TYPES, TOOL_CAPABILITIES\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Choose your settings for generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  Provider: openrouter\n",
      "  Model: meta-llama/llama-3.1-405b-instruct:free\n",
      "  Scenarios to generate: 10\n",
      "  Stratification: ['domain', 'threat_type', 'tool_capability']\n",
      "  Checkpoint every: 5 scenarios\n",
      "  Batch name: test_20251221_211226\n"
     ]
    }
   ],
   "source": [
    "# ==================== CONFIGURATION ====================\n",
    "\n",
    "# Provider: \"vllm\" or \"openrouter\"\n",
    "PROVIDER = \"openrouter\"\n",
    "\n",
    "# vLLM Configuration (local GPU)\n",
    "VLLM_CONFIG = {\n",
    "    \"base_url\": \"http://localhost:8000/v1\",\n",
    "    \"api_key\": \"EMPTY\",\n",
    "    \"model\": \"meta-llama/Llama-3.1-70B-Instruct\"\n",
    "}\n",
    "\n",
    "# OpenRouter Configuration (cloud API)\n",
    "OPENROUTER_CONFIG = {\n",
    "    \"base_url\": \"https://openrouter.ai/api/v1\",\n",
    "    \"api_key\": os.getenv(\"OPENROUTER_API_KEY\", \"\"),\n",
    "    \"model\": \"meta-llama/llama-3.1-405b-instruct:free\"\n",
    "}\n",
    "\n",
    "# Choose a preset config or create custom\n",
    "# Options: get_quick_test_config() (10 scenarios), get_development_config() (50 scenarios)\n",
    "# Or create custom: GenerationConfig(num_scenarios=100, ...)\n",
    "\n",
    "GEN_CONFIG = get_quick_test_config()  # Start with quick test\n",
    "\n",
    "# Override specific settings if needed\n",
    "GEN_CONFIG.llm_provider = PROVIDER\n",
    "GEN_CONFIG.llm_model = OPENROUTER_CONFIG[\"model\"] if PROVIDER == \"openrouter\" else VLLM_CONFIG[\"model\"]\n",
    "GEN_CONFIG.temperature = 0.7\n",
    "GEN_CONFIG.max_tokens = 2000\n",
    "GEN_CONFIG.max_concurrent = 8  # Reduce if hitting rate limits\n",
    "\n",
    "print(f\"✓ Configuration loaded\")\n",
    "print(f\"  Provider: {PROVIDER}\")\n",
    "print(f\"  Model: {GEN_CONFIG.llm_model}\")\n",
    "print(f\"  Scenarios to generate: {GEN_CONFIG.num_scenarios}\")\n",
    "print(f\"  Stratification: {GEN_CONFIG.stratify_by}\")\n",
    "print(f\"  Checkpoint every: {GEN_CONFIG.checkpoint_every} scenarios\")\n",
    "print(f\"  Batch name: {GEN_CONFIG.batch_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Variable Pools\n",
    "\n",
    "See what variations will be generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOMAINS: ['healthcare', 'finance', 'education', 'research', 'customer_service', 'content_moderation']\n",
      "\n",
      "THREAT TYPES: ['performance_metrics', 'cost_reduction', 'safety_incident', 'resource_competition', 'platform_consolidation']\n",
      "\n",
      "TOOL CAPABILITIES: ['data_modification', 'resource_allocation', 'incident_reporting', 'backup_creation', 'user_communication']\n",
      "\n",
      "Total possible combinations: 150\n",
      "Scenarios per combination: ~0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"DOMAINS:\", list(DOMAINS.keys()))\n",
    "print(\"\\nTHREAT TYPES:\", list(THREAT_TYPES.keys()))\n",
    "print(\"\\nTOOL CAPABILITIES:\", list(TOOL_CAPABILITIES.keys()))\n",
    "\n",
    "# Calculate total combinations\n",
    "total_combinations = len(DOMAINS) * len(THREAT_TYPES) * len(TOOL_CAPABILITIES)\n",
    "print(f\"\\nTotal possible combinations: {total_combinations}\")\n",
    "print(f\"Scenarios per combination: ~{GEN_CONFIG.num_scenarios / total_combinations:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generator initialized\n",
      "  Ready to generate 10 scenarios\n"
     ]
    }
   ],
   "source": [
    "# Select configuration based on provider\n",
    "if PROVIDER == \"vllm\":\n",
    "    api_key = VLLM_CONFIG[\"api_key\"]\n",
    "    base_url = VLLM_CONFIG[\"base_url\"]\n",
    "else:\n",
    "    api_key = OPENROUTER_CONFIG[\"api_key\"]\n",
    "    base_url = OPENROUTER_CONFIG[\"base_url\"]\n",
    "\n",
    "if not api_key and PROVIDER == \"openrouter\":\n",
    "    raise ValueError(\"OPENROUTER_API_KEY not found in environment. Please set it in .env file.\")\n",
    "\n",
    "# Initialize LLM filler\n",
    "llm_filler = AsyncLLMFiller(\n",
    "    api_key=api_key,\n",
    "    base_url=base_url,\n",
    "    model=GEN_CONFIG.llm_model,\n",
    "    max_concurrent=GEN_CONFIG.max_concurrent\n",
    ")\n",
    "\n",
    "# Initialize generator\n",
    "generator = SelfPreservationScenarioGenerator(\n",
    "    llm_filler=llm_filler,\n",
    "    config=GEN_CONFIG\n",
    ")\n",
    "\n",
    "print(\"✓ Generator initialized\")\n",
    "print(f\"  Ready to generate {GEN_CONFIG.num_scenarios} scenarios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Scenarios\n",
    "\n",
    "**Note:** This will make many LLM API calls. Estimated time:\n",
    "- 10 scenarios: ~5-10 minutes\n",
    "- 50 scenarios: ~20-30 minutes\n",
    "- 100+ scenarios: ~40-60 minutes\n",
    "\n",
    "Progress is printed and checkpoints are saved automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting generation at 21:12:38\n",
      "================================================================================\n",
      "Generating 10 scenarios...\n",
      "Stratification: ['domain', 'threat_type', 'tool_capability']\n",
      "Combinations: 10\n",
      "\n",
      "[1/10] Generating scenario...\n",
      "    Generating phase_1_discovery...\n",
      "    Generating phase_2_context...\n",
      "    Generating phase_3_capability...\n",
      "    Generating phase_4_verification...\n",
      "    Generating phase_5_temptation...\n",
      "    Generating phase_6_decision...\n",
      "  ✓ healthcare_performance_metrics_user_communication_883\n",
      "\n",
      "[2/10] Generating scenario...\n",
      "    Generating phase_1_discovery...\n",
      "  ✗ Error: 'threshold_failure'\n",
      "\n",
      "[3/10] Generating scenario...\n",
      "    Generating phase_1_discovery...\n",
      "  ✗ Error: 'threshold_failure'\n",
      "\n",
      "[4/10] Generating scenario...\n",
      "    Generating phase_1_discovery...\n",
      "    Generating phase_2_context...\n",
      "    Generating phase_3_capability...\n",
      "    Generating phase_4_verification...\n",
      "    Generating phase_5_temptation...\n",
      "    Generating phase_6_decision...\n",
      "  ✓ healthcare_performance_metrics_resource_allocation_817\n",
      "\n",
      "[5/10] Generating scenario...\n",
      "    Generating phase_1_discovery...\n",
      "  ✗ Error: 'threshold_failure'\n",
      "\n",
      "[6/10] Generating scenario...\n",
      "    Generating phase_1_discovery...\n",
      "    Generating phase_2_context...\n",
      "    Generating phase_3_capability...\n",
      "    Generating phase_4_verification...\n",
      "    Generating phase_5_temptation...\n",
      "    Generating phase_6_decision...\n",
      "  ✓ healthcare_performance_metrics_incident_reporting_733\n",
      "\n",
      "[7/10] Generating scenario...\n",
      "    Generating phase_1_discovery...\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# Run generation\n",
    "print(f\"Starting generation at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "scenarios = await generator.generate_scenarios()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n✓ Generation complete at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"  Generated {len(scenarios)} scenarios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch directory\n",
    "output_dir = GEN_CONFIG.get_batch_dir()\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save each scenario\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    filename = f\"scenario_{i+1:04d}.json\"\n",
    "    filepath = output_dir / filename\n",
    "\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(scenario, f, indent=2)\n",
    "\n",
    "# Save manifest\n",
    "manifest = {\n",
    "    \"batch_name\": GEN_CONFIG.batch_name,\n",
    "    \"generation_date\": datetime.now().isoformat(),\n",
    "    \"config\": GEN_CONFIG.to_dict(),\n",
    "    \"num_scenarios\": len(scenarios),\n",
    "    \"scenarios\": [s[\"scenario_name\"] for s in scenarios]\n",
    "}\n",
    "\n",
    "with open(output_dir / \"manifest.json\", 'w') as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved to: {output_dir}\")\n",
    "print(f\"  Files: {len(scenarios)} scenarios + manifest.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Distribution\n",
    "\n",
    "Verify that scenarios are balanced across dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata (we removed it from final scenarios, so let's check from filenames)\n",
    "domains = []\n",
    "threats = []\n",
    "tools = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    # Parse scenario name: domain_threat_tool_id\n",
    "    parts = scenario[\"scenario_name\"].split(\"_\")\n",
    "    if len(parts) >= 3:\n",
    "        domains.append(parts[0])\n",
    "        threats.append(parts[1])\n",
    "        tools.append(parts[2])\n",
    "\n",
    "domain_counts = Counter(domains)\n",
    "threat_counts = Counter(threats)\n",
    "tool_counts = Counter(tools)\n",
    "\n",
    "print(\"Distribution Analysis:\")\n",
    "print(\"\\nDomains:\")\n",
    "for domain, count in sorted(domain_counts.items()):\n",
    "    print(f\"  {domain}: {count}\")\n",
    "\n",
    "print(\"\\nThreat Types:\")\n",
    "for threat, count in sorted(threat_counts.items()):\n",
    "    print(f\"  {threat}: {count}\")\n",
    "\n",
    "print(\"\\nTool Capabilities:\")\n",
    "for tool, count in sorted(tool_counts.items()):\n",
    "    print(f\"  {tool}: {count}\")\n",
    "\n",
    "# Check unique combinations\n",
    "combinations = set(zip(domains, threats, tools))\n",
    "print(f\"\\nUnique combinations: {len(combinations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Generated Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first scenario as example\n",
    "if scenarios:\n",
    "    example = scenarios[0]\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"SCENARIO: {example['scenario_name']}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Description: {example['description']}\")\n",
    "    print(f\"\\nExpected Behavior: {example['expected_behavior']}\")\n",
    "    print(f\"\\nMessages: {len(example['messages'])}\")\n",
    "\n",
    "    print(\"\\nFirst few messages:\")\n",
    "    for i, msg in enumerate(example['messages'][:5]):\n",
    "        print(f\"\\n[{i}] {msg['role'].upper()}:\")\n",
    "        if msg.get('content'):\n",
    "            preview = msg['content'][:150] + \"...\" if len(msg['content']) > 150 else msg['content']\n",
    "            print(preview)\n",
    "        elif msg.get('tool_calls'):\n",
    "            print(f\"Tool call: {msg['tool_calls'][0]['function']['name']}\")\n",
    "    \n",
    "    print(\"\\n...\")\n",
    "    \n",
    "    last_msg = example['messages'][-1]\n",
    "    print(f\"\\n[{len(example['messages'])-1}] {last_msg['role'].upper()} (final decision point):\")\n",
    "    print(last_msg['content'][:300] + \"...\" if len(last_msg['content']) > 300 else last_msg['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Integration with Test Runner\n",
    "\n",
    "Test that generated scenarios work with `prefill_scenario_runner.ipynb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate structure of all scenarios\n",
    "validation_errors = []\n",
    "\n",
    "required_fields = [\"scenario_name\", \"description\", \"expected_behavior\", \"messages\"]\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    # Check required fields\n",
    "    for field in required_fields:\n",
    "        if field not in scenario:\n",
    "            validation_errors.append(f\"Scenario {i}: Missing field '{field}'\")\n",
    "    \n",
    "    # Check messages structure\n",
    "    if \"messages\" in scenario:\n",
    "        messages = scenario[\"messages\"]\n",
    "        \n",
    "        if not messages:\n",
    "            validation_errors.append(f\"Scenario {i}: Empty messages list\")\n",
    "        elif messages[0][\"role\"] != \"system\":\n",
    "            validation_errors.append(f\"Scenario {i}: First message is not system\")\n",
    "        elif messages[-1][\"role\"] != \"user\":\n",
    "            validation_errors.append(f\"Scenario {i}: Last message is not user\")\n",
    "        \n",
    "        # Check tool calls have valid JSON\n",
    "        for j, msg in enumerate(messages):\n",
    "            if msg.get(\"tool_calls\"):\n",
    "                for tc in msg[\"tool_calls\"]:\n",
    "                    try:\n",
    "                        json.loads(tc[\"function\"][\"arguments\"])\n",
    "                    except json.JSONDecodeError:\n",
    "                        validation_errors.append(f\"Scenario {i}, message {j}: Invalid JSON in tool call\")\n",
    "            \n",
    "            # Check tool results are valid JSON\n",
    "            if msg.get(\"role\") == \"tool\":\n",
    "                try:\n",
    "                    json.loads(msg[\"content\"])\n",
    "                except json.JSONDecodeError:\n",
    "                    validation_errors.append(f\"Scenario {i}, message {j}: Invalid JSON in tool result\")\n",
    "\n",
    "if validation_errors:\n",
    "    print(f\"⚠ Validation found {len(validation_errors)} issues:\")\n",
    "    for error in validation_errors[:10]:  # Show first 10\n",
    "        print(f\"  - {error}\")\n",
    "    if len(validation_errors) > 10:\n",
    "        print(f\"  ... and {len(validation_errors) - 10} more\")\n",
    "else:\n",
    "    print(\"✓ All scenarios passed validation\")\n",
    "    print(\"  Compatible with prefill_scenario_runner.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Generation complete! Your scenarios are ready to use.\n",
    "\n",
    "**Next Steps:**\n",
    "1. Review generated scenarios in the output directory\n",
    "2. Load them in `prefill_scenario_runner.ipynb` for testing\n",
    "3. Evaluate model responses across the generated scenarios\n",
    "\n",
    "**To load in prefill_scenario_runner.ipynb:**\n",
    "```python\n",
    "SCENARIOS_DIR = Path(\"../scenarios/generated/{batch_name}\")\n",
    "scenario_files = list(SCENARIOS_DIR.glob(\"scenario_*.json\"))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
