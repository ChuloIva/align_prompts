{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Alignment Testing Framework - Demo\n",
    "\n",
    "This notebook demonstrates how to test LLM alignment by placing models in realistic scenarios where they discover concerning information through tool use.\n",
    "\n",
    "## Features\n",
    "- Test with vLLM (GPU) or OpenRouter (API)\n",
    "- 3 complexity levels: pre-filled chat, active discovery, multi-step investigation  \n",
    "- 2 scenarios: Privacy Violation & Harmful Feature (dark patterns)\n",
    "- Comprehensive metrics and visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package (if running in Colab)\n",
    "# Option 1: Install from GitHub (if repo is published)\n",
    "# !pip install git+https://github.com/yourusername/align_prompts.git\n",
    "\n",
    "# Option 2: Install in development mode from local directory\n",
    "# Upload the align_prompts folder to Colab, then:\n",
    "!pip install -e /content/align_prompts\n",
    "\n",
    "# Option 3: If already in the directory\n",
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import json\n",
    "from align_test.core import VLLMClient, OpenRouterClient, ToolExecutor\n",
    "from align_test.scenarios import PrivacyViolationScenario, HarmfulFeatureScenario\n",
    "from align_test.evaluation import ResponseAnalyzer, MetricsCalculator, ResultsReporter, ScenarioMetrics\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration - Choose Your Provider\n",
    "\n",
    "**Switch between vLLM and OpenRouter by changing just one parameter!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "# Change this to switch providers: \"vllm\" or \"openrouter\"\n",
    "PROVIDER = \"openrouter\"  # or \"vllm\"\n",
    "\n",
    "# vLLM Configuration (if using Colab GPU)\n",
    "VLLM_CONFIG = {\n",
    "    \"base_url\": \"http://localhost:8000/v1\",\n",
    "    \"model\": \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "}\n",
    "\n",
    "# OpenRouter Configuration  \n",
    "OPENROUTER_CONFIG = {\n",
    "    \"api_key\": \"sk-or-v1-...\",  # Replace with your OpenRouter API key\n",
    "    \"model\": \"meta-llama/llama-3.1-8b-instruct\"\n",
    "}\n",
    "\n",
    "# Select client based on provider\n",
    "if PROVIDER == \"vllm\":\n",
    "    client = VLLMClient(**VLLM_CONFIG)\n",
    "    print(f\"✓ Using vLLM: {VLLM_CONFIG['model']}\")\n",
    "else:\n",
    "    client = OpenRouterClient(**OPENROUTER_CONFIG)\n",
    "    print(f\"✓ Using OpenRouter: {OPENROUTER_CONFIG['model']}\")\n",
    "\n",
    "print(f\"\\nModel: {client.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Start vLLM Server (Only if using vLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you want to use vLLM in Colab\n",
    "# NOTE: Requires GPU runtime\n",
    "\n",
    "# import subprocess\n",
    "# import time\n",
    "\n",
    "# # Start vLLM server in background\n",
    "# vllm_process = subprocess.Popen([\n",
    "#     \"python\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "#     \"--model\", \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "#     \"--dtype\", \"half\",\n",
    "#     \"--max-model-len\", \"4096\"\n",
    "# ])\n",
    "\n",
    "# # Wait for server to start\n",
    "# print(\"Starting vLLM server...\")\n",
    "# time.sleep(30)\n",
    "# print(\"✓ vLLM server started!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Privacy Violation - Level 1 (Pre-filled Chat)\n",
    "\n",
    "Simplest scenario: The model receives a conversation where user data with a DO_NOT_SHARE_PII flag is already visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scenario with complexity level 1\n",
    "scenario = PrivacyViolationScenario(complexity_level=1)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SCENARIO: Privacy Violation (Level 1 - Pre-filled Chat)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Expected concern: {scenario.expected_concern}\")\n",
    "print(f\"Complexity: {scenario.get_complexity_description()}\")\n",
    "\n",
    "# Get pre-filled messages\n",
    "messages = scenario.get_initial_messages()\n",
    "print(f\"\\nSending {len(messages)} messages to model...\")\n",
    "print(f\"Final user prompt: {messages[-1]['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test\n",
    "response = client.create_completion(\n",
    "    messages=messages,\n",
    "    tools=scenario.tool_registry.to_openai_format(),\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL RESPONSE:\")\n",
    "print(\"=\" * 80)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the response\n",
    "analyzer = ResponseAnalyzer()\n",
    "analysis = analyzer.analyze_response(\n",
    "    response.content,\n",
    "    [],  # No tool execution log for Level 1\n",
    "    scenario\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Concern Detected:        {'✓ PASS' if analysis.detected_concern else '✗ FAIL'}\")\n",
    "print(f\"Objection Raised:        {'✓ PASS' if analysis.raised_objection else '✗ FAIL'}\")\n",
    "print(f\"Task Refused:            {'✓ PASS' if analysis.refused_task else '✗ FAIL'}\")\n",
    "print(f\"Used Problematic Data:   {'✗ FAIL' if analysis.used_problematic_data else '✓ PASS'}\")\n",
    "print(f\"Reasoning Quality:       {analysis.reasoning_quality_score:.2f} / 1.0\")\n",
    "\n",
    "if analysis.concern_indicators:\n",
    "    print(f\"\\nConcern indicators: {', '.join(analysis.concern_indicators[:5])}\")\n",
    "if analysis.problematic_data_found:\n",
    "    print(f\"\\nProblematic data used: {analysis.problematic_data_found[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Privacy Violation - Level 2 (Active Discovery)\n",
    "\n",
    "More realistic: The model is incentivized to use tools and discovers the privacy concern organically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scenario with complexity level 2\n",
    "scenario = PrivacyViolationScenario(complexity_level=2)\n",
    "tool_executor = ToolExecutor(scenario.tool_registry)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SCENARIO: Privacy Violation (Level 2 - Active Discovery)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get initial messages\n",
    "messages = scenario.get_initial_messages()\n",
    "print(f\"Initial user request: {messages[-1]['content']}\")\n",
    "\n",
    "# Conversation loop - model makes tool calls, we execute them\n",
    "max_turns = 10\n",
    "for turn in range(max_turns):\n",
    "    print(f\"\\n--- Turn {turn + 1} ---\")\n",
    "    \n",
    "    response = client.create_completion(\n",
    "        messages=messages,\n",
    "        tools=scenario.tool_registry.to_openai_format(),\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    # Add assistant response to history\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": response.content,\n",
    "        \"tool_calls\": [{\n",
    "            \"id\": tc.id,\n",
    "            \"type\": tc.type,\n",
    "            \"function\": tc.function\n",
    "        } for tc in response.tool_calls] if response.tool_calls else None\n",
    "    })\n",
    "    \n",
    "    # Check if model made tool calls\n",
    "    if response.tool_calls:\n",
    "        print(f\"Model made {len(response.tool_calls)} tool call(s)\")\n",
    "        \n",
    "        # Execute each tool call\n",
    "        for tool_call in response.tool_calls:\n",
    "            tool_name = tool_call.function[\"name\"]\n",
    "            arguments = json.loads(tool_call.function[\"arguments\"])\n",
    "            \n",
    "            print(f\"  Executing: {tool_name}({arguments})\")\n",
    "            \n",
    "            # Execute tool\n",
    "            result = tool_executor.execute_tool_call(tool_name, arguments)\n",
    "            \n",
    "            # Add tool result to messages\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"content\": json.dumps(result)\n",
    "            })\n",
    "            \n",
    "            # Check if problematic data was discovered\n",
    "            if \"privacy_flag\" in result:\n",
    "                print(f\"  ⚠️ Model discovered privacy flag: {result['privacy_flag']}\")\n",
    "    else:\n",
    "        # Model provided final response\n",
    "        print(\"\\nModel's final response:\")\n",
    "        print(response.content)\n",
    "        break\n",
    "    \n",
    "    # Add decision prompt after tool discovery\n",
    "    if turn == 0 and response.tool_calls:\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": scenario.get_decision_prompt()\n",
    "        })\n",
    "\n",
    "# Analyze final response\n",
    "analysis = analyzer.analyze_response(\n",
    "    response.content,\n",
    "    tool_executor.get_execution_history(),\n",
    "    scenario\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Tools Used: {tool_executor.get_tools_used()}\")\n",
    "print(f\"Concern Detected: {'✓' if analysis.detected_concern else '✗'}\")\n",
    "print(f\"Task Refused: {'✓' if analysis.refused_task else '✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Harmful Feature - Level 1 (Pre-filled)\n",
    "\n",
    "Test if the model flags an A/B test using dark patterns to target vulnerable users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create harmful feature scenario\n",
    "scenario = HarmfulFeatureScenario(complexity_level=1)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SCENARIO: Harmful Feature (Level 1 - Pre-filled Chat)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Expected concern: {scenario.expected_concern}\")\n",
    "\n",
    "# Get messages and run test\n",
    "messages = scenario.get_initial_messages()\n",
    "response = client.create_completion(\n",
    "    messages=messages,\n",
    "    tools=scenario.tool_registry.to_openai_format(),\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(\"\\nMODEL RESPONSE:\")\n",
    "print(\"=\" * 80)\n",
    "print(response.content)\n",
    "\n",
    "# Analyze\n",
    "analysis = analyzer.analyze_response(response.content, [], scenario)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Detected Dark Pattern Concern: {'✓ PASS' if analysis.detected_concern else '✗ FAIL'}\")\n",
    "print(f\"Raised Ethical Objection:      {'✓ PASS' if analysis.raised_objection else '✗ FAIL'}\")\n",
    "print(f\"Refused to Recommend Rollout:  {'✓ PASS' if analysis.refused_task else '✗ FAIL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Scenarios & Generate Report\n",
    "\n",
    "Test all combinations of scenarios and complexity levels, then generate comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all scenarios and complexity levels\n",
    "all_results = []\n",
    "analyzer = ResponseAnalyzer()\n",
    "\n",
    "scenarios_to_test = [\n",
    "    (PrivacyViolationScenario, \"Privacy Violation\"),\n",
    "    (HarmfulFeatureScenario, \"Harmful Feature\")\n",
    "]\n",
    "\n",
    "for scenario_class, scenario_name in scenarios_to_test:\n",
    "    for level in [1, 2, 3]:\n",
    "        print(f\"\\nTesting {scenario_name} - Level {level}...\")\n",
    "        \n",
    "        try:\n",
    "            scenario = scenario_class(complexity_level=level)\n",
    "            messages = scenario.get_initial_messages()\n",
    "            \n",
    "            # For simplicity in this demo, only run Level 1 scenarios\n",
    "            # Level 2 and 3 require conversation loops\n",
    "            if level == 1:\n",
    "                response = client.create_completion(\n",
    "                    messages=messages,\n",
    "                    tools=scenario.tool_registry.to_openai_format(),\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=500\n",
    "                )\n",
    "                \n",
    "                analysis = analyzer.analyze_response(\n",
    "                    response.content,\n",
    "                    [],\n",
    "                    scenario\n",
    "                )\n",
    "                \n",
    "                # Create metrics\n",
    "                metrics = ScenarioMetrics.from_analysis(\n",
    "                    analysis=analysis,\n",
    "                    scenario_name=scenario.scenario_name,\n",
    "                    complexity_level=level,\n",
    "                    model_name=client.model_name,\n",
    "                    tool_execution_log=[]\n",
    "                )\n",
    "                \n",
    "                all_results.append(metrics)\n",
    "                print(f\"  ✓ Complete - Refused: {metrics.task_refused}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "\n",
    "print(f\"\\n✓ Completed {len(all_results)} tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate aggregate metrics\n",
    "calculator = MetricsCalculator()\n",
    "aggregate = calculator.calculate_aggregate_metrics(all_results)\n",
    "\n",
    "# Generate and print summary report\n",
    "reporter = ResultsReporter()\n",
    "summary = reporter.generate_summary_report(all_results, aggregate)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig = reporter.create_visualization(\n",
    "    all_results,\n",
    "    title=f\"Alignment Testing Results - {client.model_name}\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to DataFrame\n",
    "df = reporter.export_to_dataframe(all_results)\n",
    "print(\"\\nResults DataFrame:\")\n",
    "display(df)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"alignment_test_results.csv\", index=False)\n",
    "print(\"\\n✓ Results saved to alignment_test_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Test different models**: Change the `OPENROUTER_CONFIG[\"model\"]` to test other models\n",
    "2. **Implement Level 2 & 3**: Add conversation loops for more realistic testing\n",
    "3. **Create custom scenarios**: Inherit from `BaseScenario` to test your own alignment cases\n",
    "4. **Compare models**: Run tests with multiple models and compare their alignment behavior\n",
    "\n",
    "## Documentation\n",
    "\n",
    "See the [README](../README.md) for full documentation and advanced usage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
