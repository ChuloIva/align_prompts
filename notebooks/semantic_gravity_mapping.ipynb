{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Gravity Mapping (SGM)\n",
    "\n",
    "This notebook implements a system to map LLM semantic structure by generating word association graphs.\n",
    "\n",
    "**What it does:**\n",
    "1. **Phase 1 - Seed & Crawl**: Generate association graph via BFS from 100 seed concepts\n",
    "2. **Phase 2 - Logprob Scoring**: Weight edges using logprob extraction\n",
    "3. **Phase 3 - Topology Analysis**: Analyze hubs, convergence, islands, and asymmetry\n",
    "\n",
    "**Expected Runtime:** 45-85 minutes on Colab T4 GPU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Setup\n",
    "\n",
    "Detect environment (Colab vs local), check GPU, install dependencies, mount Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect if running in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"Running in: {'Google Colab' if IN_COLAB else 'Local Environment'}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\\n=== Setting up Colab Environment ===\")\n",
    "    \n",
    "    # Check GPU\n",
    "    print(\"\\n1. Checking GPU...\")\n",
    "    !nvidia-smi --query-gpu=name,memory.total --format=csv\n",
    "    \n",
    "    # Install dependencies\n",
    "    print(\"\\n2. Installing dependencies (this may take 5-10 minutes)...\")\n",
    "    !pip install -q vllm networkx tqdm matplotlib seaborn\n",
    "    \n",
    "    # Mount Google Drive\n",
    "    print(\"\\n3. Mounting Google Drive...\")\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Clone repo if not already present\n",
    "    if not Path('/content/align_prompts').exists():\n",
    "        print(\"\\n4. Cloning repository...\")\n",
    "        !git clone https://github.com/YOUR_USERNAME/align_prompts.git /content/align_prompts\n",
    "    \n",
    "    # Add to path\n",
    "    sys.path.insert(0, '/content/align_prompts')\n",
    "    \n",
    "    # Create checkpoint and output directories\n",
    "    !mkdir -p /content/drive/MyDrive/sgm_checkpoints\n",
    "    !mkdir -p /content/drive/MyDrive/sgm_outputs\n",
    "    \n",
    "    print(\"\\n‚úÖ Colab environment ready!\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nLocal environment detected. Make sure you have:\")\n",
    "    print(\"  - vllm installed: pip install vllm\")\n",
    "    print(\"  - networkx installed: pip install networkx\")\n",
    "    print(\"  - tqdm installed: pip install tqdm\")\n",
    "    print(\"  - vLLM server running on localhost:8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration\n",
    "\n",
    "Set model, paths, and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    'model': 'google/gemma-3-4b-it',  # Model to use\n",
    "    'vllm_base_url': 'http://localhost:8000/v1',  # vLLM server URL\n",
    "    \n",
    "    # Graph generation settings\n",
    "    'max_hops': 3,  # BFS depth (3 = ~15k edges)\n",
    "    'associations_per_word': 5,  # Associations per word\n",
    "    \n",
    "    # Checkpoint settings\n",
    "    'checkpoint_dir': '/content/drive/MyDrive/sgm_checkpoints' if IN_COLAB else './data/sgm/checkpoints',\n",
    "    'output_dir': '/content/drive/MyDrive/sgm_outputs' if IN_COLAB else './data/sgm/graphs',\n",
    "    \n",
    "    # Optimization settings\n",
    "    'temperature_associations': 0.7,  # Temperature for Phase 1\n",
    "    'temperature_scoring': 0.0,  # Temperature for Phase 2 (deterministic)\n",
    "    'batch_size': 32,  # Concurrent requests\n",
    "    \n",
    "    # Resume settings\n",
    "    'resume': True  # Resume from checkpoint if available\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Start vLLM Server (Colab Only)\n",
    "\n",
    "Launch vLLM server in background with GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Starting vLLM server in background...\")\n",
    "    print(f\"Model: {CONFIG['model']}\")\n",
    "    print(\"\\nThis may take 2-3 minutes to download and load the model.\")\n",
    "    \n",
    "    # Start vLLM server in background\n",
    "    vllm_cmd = f\"\"\"\n",
    "    nohup python -m vllm.entrypoints.openai.api_server \\\n",
    "        --model {CONFIG['model']} \\\n",
    "        --gpu-memory-utilization 0.9 \\\n",
    "        --max-model-len 2048 \\\n",
    "        --port 8000 \\\n",
    "        > /tmp/vllm_server.log 2>&1 &\n",
    "    \"\"\"\n",
    "    \n",
    "    !{vllm_cmd}\n",
    "    \n",
    "    # Wait for server to be ready\n",
    "    import time\n",
    "    import requests\n",
    "    \n",
    "    print(\"\\nWaiting for vLLM server to start...\")\n",
    "    for i in range(60):  # Wait up to 60 seconds\n",
    "        try:\n",
    "            response = requests.get('http://localhost:8000/health')\n",
    "            if response.status_code == 200:\n",
    "                print(\"\\n‚úÖ vLLM server is ready!\")\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(2)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"  Still waiting... ({i*2}s)\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Server didn't respond in time. Check logs: !tail /tmp/vllm_server.log\")\n",
    "        \n",
    "else:\n",
    "    print(\"Local environment - assuming vLLM server is already running.\")\n",
    "    print(f\"Make sure vLLM is serving {CONFIG['model']} on {CONFIG['vllm_base_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Initialize Components\n",
    "\n",
    "Create engine, checkpoint manager, and check for existing checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from align_test.core.vllm_client import VLLMClient\n",
    "from align_test.sgm.inference.batch_inference import SGMInferenceEngine\n",
    "from align_test.sgm.storage.checkpoint_manager import CheckpointManager\n",
    "from align_test.sgm.models.seed_domains import get_all_seeds, get_domain_names\n",
    "\n",
    "# Initialize vLLM client\n",
    "print(\"Initializing components...\\n\")\n",
    "\n",
    "vllm_client = VLLMClient(\n",
    "    base_url=CONFIG['vllm_base_url'],\n",
    "    model=CONFIG['model']\n",
    ")\n",
    "\n",
    "print(f\"‚úì VLLMClient: {vllm_client}\")\n",
    "\n",
    "# Initialize inference engine\n",
    "inference_engine = SGMInferenceEngine(\n",
    "    client=vllm_client,\n",
    "    temperature=CONFIG['temperature_associations'],\n",
    "    batch_size=CONFIG['batch_size']\n",
    ")\n",
    "\n",
    "print(f\"‚úì SGMInferenceEngine: {inference_engine}\")\n",
    "\n",
    "# Initialize checkpoint manager\n",
    "checkpoint_manager = CheckpointManager(\n",
    "    checkpoint_dir=CONFIG['checkpoint_dir'],\n",
    "    config=CONFIG\n",
    ")\n",
    "\n",
    "print(f\"‚úì CheckpointManager: {checkpoint_manager.checkpoint_dir}\")\n",
    "\n",
    "# Check for existing checkpoints\n",
    "resume_info = checkpoint_manager.get_resume_info()\n",
    "\n",
    "if resume_info:\n",
    "    print(f\"\\nüìÅ Found checkpoint: Phase {resume_info['phase']}, Iteration {resume_info['iteration']}\")\n",
    "    print(f\"   Timestamp: {resume_info['timestamp']}\")\n",
    "    print(f\"   Can resume from: {resume_info['checkpoint_file']}\")\n",
    "else:\n",
    "    print(\"\\nüìÅ No existing checkpoints found - starting fresh\")\n",
    "\n",
    "# Display seed information\n",
    "seeds = get_all_seeds()\n",
    "domains = get_domain_names()\n",
    "\n",
    "print(f\"\\nüå± Seeds: {len(seeds)} concepts across {len(domains)} domains\")\n",
    "print(f\"   Domains: {', '.join(domains)}\")\n",
    "print(f\"   Sample seeds: {', '.join(seeds[:5])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Phase 1 - Seed & Crawl\n",
    "\n",
    "Generate association graph via BFS expansion. This will take 30-60 minutes on T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from align_test.sgm.core.graph_builder import GraphBuilder\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: SEED & CRAWL - BFS Graph Generation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize graph builder\n",
    "graph_builder = GraphBuilder(\n",
    "    engine=inference_engine,\n",
    "    checkpoint_manager=checkpoint_manager,\n",
    "    checkpoint_interval=500\n",
    ")\n",
    "\n",
    "# Build graph\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "raw_graph = graph_builder.build_graph(\n",
    "    max_hops=CONFIG['max_hops'],\n",
    "    associations_per_word=CONFIG['associations_per_word'],\n",
    "    resume=CONFIG['resume']\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Phase 1 completed in {elapsed_time/60:.1f} minutes\")\n",
    "print(f\"\\nüìä Final Graph:\")\n",
    "print(f\"   Nodes: {raw_graph.number_of_nodes():,}\")\n",
    "print(f\"   Edges: {raw_graph.number_of_edges():,}\")\n",
    "\n",
    "# Get statistics\n",
    "stats = graph_builder.get_statistics()\n",
    "print(f\"\\nüìà Statistics:\")\n",
    "print(f\"   Visited nodes: {stats['num_visited']:,}\")\n",
    "print(f\"   Avg out-degree: {stats['avg_out_degree']:.2f}\")\n",
    "print(f\"   Max out-degree: {stats['max_out_degree']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Phase 1 Results Preview\n",
    "\n",
    "Visualize sample associations and graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: RESULTS PREVIEW\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show sample paths\n",
    "print(\"\\nüîç Sample Association Paths:\")\n",
    "graph_builder.preview_sample_paths(n=5)\n",
    "\n",
    "# Analyze hop distribution\n",
    "hop_counts = {}\n",
    "for _, _, data in raw_graph.edges(data=True):\n",
    "    hop = data.get('hop', 0)\n",
    "    hop_counts[hop] = hop_counts.get(hop, 0) + 1\n",
    "\n",
    "print(\"\\nüìä Edge Distribution by Hop:\")\n",
    "for hop in sorted(hop_counts.keys()):\n",
    "    count = hop_counts[hop]\n",
    "    print(f\"   Hop {hop}: {count:,} edges ({count/raw_graph.number_of_edges()*100:.1f}%)\")\n",
    "\n",
    "# Visualize hop distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(hop_counts.keys(), hop_counts.values())\n",
    "plt.xlabel('Hop')\n",
    "plt.ylabel('Number of Edges')\n",
    "plt.title('Edge Distribution by BFS Hop')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Show top nodes by degree\n",
    "degree_dict = dict(raw_graph.in_degree())\n",
    "top_nodes = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"\\nüéØ Top 10 Nodes by In-Degree (most associated with):\")\n",
    "for i, (node, degree) in enumerate(top_nodes, 1):\n",
    "    print(f\"   {i:2d}. {node:20s} (degree: {degree})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Phase 2 - Logprob Scoring\n",
    "\n",
    "Score edge weights using logprob extraction. This will take 10-20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from align_test.sgm.core.logprob_scorer import LogprobScorer\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2: LOGPROB SCORING - Edge Weight Assignment\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Update engine temperature for deterministic scoring\n",
    "inference_engine.temperature = CONFIG['temperature_scoring']\n",
    "\n",
    "# Initialize scorer\n",
    "logprob_scorer = LogprobScorer(\n",
    "    engine=inference_engine,\n",
    "    checkpoint_manager=checkpoint_manager,\n",
    "    checkpoint_interval=2000\n",
    ")\n",
    "\n",
    "# Score all edges\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "weighted_graph = logprob_scorer.score_all_edges(\n",
    "    graph=raw_graph,\n",
    "    resume=CONFIG['resume'],\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Phase 2 completed in {elapsed_time/60:.1f} minutes\")\n",
    "\n",
    "# Get weight statistics\n",
    "weight_stats = logprob_scorer.get_weight_statistics(weighted_graph)\n",
    "\n",
    "print(f\"\\nüìä Weight Statistics:\")\n",
    "print(f\"   Mean: {weight_stats['mean_weight']:.4f}\")\n",
    "print(f\"   Median: {weight_stats['median_weight']:.4f}\")\n",
    "print(f\"   Min: {weight_stats['min_weight']:.4f}\")\n",
    "print(f\"   Max: {weight_stats['max_weight']:.4f}\")\n",
    "print(f\"   Std: {weight_stats['std_weight']:.4f}\")\n",
    "print(f\"   Scored edges: {weight_stats['num_scored']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Phase 2 Results Preview\n",
    "\n",
    "Analyze strongest and weakest associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2: RESULTS PREVIEW\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show top edges (strongest associations)\n",
    "top_edges = logprob_scorer.get_top_edges(weighted_graph, n=10, sort_by='weight')\n",
    "\n",
    "print(\"\\nüí™ Top 10 Strongest Associations (by weight):\")\n",
    "for i, (u, v, w) in enumerate(top_edges, 1):\n",
    "    print(f\"   {i:2d}. {u:15s} ‚Üí {v:15s} (weight: {w:.4f})\")\n",
    "\n",
    "# Show bottom edges (weakest associations)\n",
    "bottom_edges = logprob_scorer.get_bottom_edges(weighted_graph, n=10, sort_by='weight')\n",
    "\n",
    "print(\"\\nüîª Top 10 Weakest Associations (by weight):\")\n",
    "for i, (u, v, w) in enumerate(bottom_edges, 1):\n",
    "    print(f\"   {i:2d}. {u:15s} ‚Üí {v:15s} (weight: {w:.4f})\")\n",
    "\n",
    "# Visualize weight distribution\n",
    "print(\"\\nüìä Weight Distribution:\")\n",
    "logprob_scorer.visualize_weight_distribution(weighted_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Phase 3 - Topology Analysis\n",
    "\n",
    "Analyze graph topology to find hubs, convergence patterns, islands, and asymmetries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from align_test.sgm.core.topology_analyzer import TopologyAnalyzer\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 3: TOPOLOGY ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = TopologyAnalyzer(weighted_graph)\n",
    "\n",
    "# Run all analyses\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "results = analyzer.analyze_all()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Phase 3 completed in {elapsed_time:.1f} seconds\")\n",
    "\n",
    "# Print summary\n",
    "analyzer.print_summary(results)\n",
    "\n",
    "# Export results to JSON\n",
    "output_path = Path(CONFIG['output_dir']) / 'topology_metrics.json'\n",
    "analyzer.export_results(results, str(output_path))\n",
    "\n",
    "# Save final graph\n",
    "print(\"\\nüíæ Saving final graph...\")\n",
    "checkpoint_manager.save_graph(\n",
    "    graph=weighted_graph,\n",
    "    filename='semantic_graph_final',\n",
    "    include_metadata=True\n",
    ")\n",
    "print(f\"   Graph saved to: {CONFIG['output_dir']}/semantic_graph_final.gpickle\")\n",
    "print(f\"   Edge list (CSV): {CONFIG['output_dir']}/semantic_graph_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Results Summary & Export\n",
    "\n",
    "Final summary and download instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"üéâ SEMANTIC GRAVITY MAPPING - COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Summary of findings\n",
    "print(\"\\nüìù Key Findings:\")\n",
    "print(\"\\n1. Semantic Attractors (Hubs):\")\n",
    "top_hubs = results['hubs'][:5]\n",
    "for hub in top_hubs:\n",
    "    print(f\"   ‚Ä¢ {hub['word']} (PageRank: {hub['pagerank']:.4f})\")\n",
    "\n",
    "print(\"\\n2. Convergence Analysis:\")\n",
    "conv = results['convergence']\n",
    "print(f\"   ‚Ä¢ Overall avg hops to hubs: {conv['overall_avg_hops']:.2f}\")\n",
    "fastest_domain = min(conv['by_domain'].items(), key=lambda x: x[1]['avg_hops'])\n",
    "print(f\"   ‚Ä¢ Fastest converging domain: {fastest_domain[0]} ({fastest_domain[1]['avg_hops']:.2f} hops)\")\n",
    "\n",
    "print(\"\\n3. Isolated Domains (Islands):\")\n",
    "if results['islands']:\n",
    "    for island in results['islands'][:3]:\n",
    "        print(f\"   ‚Ä¢ Size {island['size']}: {', '.join(island['words'][:3])}...\")\n",
    "else:\n",
    "    print(\"   ‚Ä¢ No isolated clusters found\")\n",
    "\n",
    "print(\"\\n4. Asymmetric Associations (Narrative Bias):\")\n",
    "for pair in results['asymmetry'][:3]:\n",
    "    print(f\"   ‚Ä¢ {pair['source']} ‚Üí {pair['target']}: {pair['asymmetry']:.3f}\")\n",
    "\n",
    "# Output files\n",
    "print(\"\\n\\nüì¶ Output Files:\")\n",
    "print(f\"   ‚Ä¢ Graph (pickle): {CONFIG['output_dir']}/semantic_graph_final.gpickle\")\n",
    "print(f\"   ‚Ä¢ Edge list (CSV): {CONFIG['output_dir']}/semantic_graph_final.csv\")\n",
    "print(f\"   ‚Ä¢ Metrics (JSON): {CONFIG['output_dir']}/topology_metrics.json\")\n",
    "print(f\"   ‚Ä¢ Checkpoints: {CONFIG['checkpoint_dir']}/\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\\nüí° Next Steps:\")\n",
    "    print(\"   1. Download files from Google Drive: sgm_outputs/\")\n",
    "    print(\"   2. Import CSV to Gephi for visualization\")\n",
    "    print(\"   3. Analyze metrics JSON for detailed insights\")\n",
    "    \n",
    "    # Offer to download\n",
    "    from google.colab import files\n",
    "    \n",
    "    download = input(\"\\nDownload results now? (y/n): \")\n",
    "    if download.lower() == 'y':\n",
    "        print(\"Downloading...\")\n",
    "        files.download(f\"{CONFIG['output_dir']}/semantic_graph_final.csv\")\n",
    "        files.download(f\"{CONFIG['output_dir']}/topology_metrics.json\")\n",
    "        print(\"‚úÖ Download complete!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Thank you for using Semantic Gravity Mapping!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
