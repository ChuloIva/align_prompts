{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Gravity Mapping (SGM)\n",
    "\n",
    "This notebook implements a system to map LLM semantic structure by generating word association graphs.\n",
    "\n",
    "**What it does:**\n",
    "1. **Phase 1 - Seed & Crawl**: Generate association graph via BFS from 100 seed concepts\n",
    "2. **Phase 2 - Logprob Scoring**: Weight edges using logprob extraction\n",
    "3. **Phase 3 - Topology Analysis**: Analyze hubs, convergence, islands, and asymmetry\n",
    "\n",
    "**Expected Runtime:** 45-85 minutes on Colab T4 GPU\n",
    "\n",
    "---\n",
    "\n",
    "## Recommended Setup: VS Code/Cursor + Colab Extension\n",
    "\n",
    "**This notebook works best with the Google Colab VS Code extension** (launched Nov 2025):\n",
    "- Keep your notebook **file local** (easy Git workflow)\n",
    "- Run code on **remote Colab GPU/TPU** (free T4 or Pro A100)\n",
    "- Use your **local IDE** (extensions, debugging, linting)\n",
    "\n",
    "**Quick Setup:**\n",
    "1. Install \"Google Colab\" extension in VS Code/Cursor\n",
    "2. Open this notebook in VS Code/Cursor\n",
    "3. Click \"Select Kernel\" ‚Üí \"Colab\" ‚Üí \"New Colab Server\"\n",
    "4. Sign in with your Google account\n",
    "5. Run the notebook - it will auto-setup the remote environment\n",
    "\n",
    "**Alternative:** You can also run this in traditional Colab web UI or locally with your own GPU.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Setup\n",
    "\n",
    "Detects environment (VS Code + Colab, traditional Colab, or local), checks GPU, installs dependencies, and syncs local code to remote runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in: Colab Runtime (GPU/TPU)\n",
      "\n",
      "=== Setting up Colab Runtime ===\n",
      "(Works with both traditional Colab and VS Code/Cursor + Colab extension)\n",
      "\n",
      "1. Checking GPU...\n",
      "name, memory.total [MiB]\n",
      "NVIDIA A100-SXM4-40GB, 40960 MiB\n",
      "\n",
      "2. Installing dependencies (5-10 minutes)...\n",
      "\n",
      "3. Setting up storage...\n",
      "   ‚ö†Ô∏è  Google Drive mounting not supported in VS Code + Colab extension\n",
      "   ‚úì  Using /content/ directory (persists during session)\n",
      "   üí° Download results at end of session\n",
      "\n",
      "4. Repository already exists. Pulling latest changes...\n",
      "Already up to date.\n",
      "   Added to sys.path: /content/align_prompts\n",
      "\n",
      "‚úÖ Colab runtime ready!\n",
      "\n",
      "üí° Important Notes:\n",
      "   ‚Ä¢ Your notebook file is local, but code runs on Colab GPU\n",
      "   ‚Ä¢ Files saved to /content/ persist during your session\n",
      "   ‚Ä¢ Download results before disconnecting (session timeout ~12hrs)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect if running in Colab runtime (includes VS Code + Colab extension)\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"Running in: {'Colab Runtime (GPU/TPU)' if IN_COLAB else 'Local Environment (CPU/GPU)'}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\\n=== Setting up Colab Runtime ===\")\n",
    "    print(\"(Works with both traditional Colab and VS Code/Cursor + Colab extension)\")\n",
    "    \n",
    "    # Check GPU\n",
    "    print(\"\\n1. Checking GPU...\")\n",
    "    !nvidia-smi --query-gpu=name,memory.total --format=csv\n",
    "    \n",
    "    # Install dependencies\n",
    "    print(\"\\n2. Installing dependencies (5-10 minutes)...\")\n",
    "    !pip install -q vllm networkx tqdm matplotlib seaborn\n",
    "    \n",
    "    # NOTE: Google Drive mounting (drive.mount) is NOT supported in VS Code + Colab extension\n",
    "    # See: https://github.com/googlecolab/colab-vscode/issues/256\n",
    "    # Workaround: Use /content/ directory which persists during the session\n",
    "    \n",
    "    print(\"\\n3. Setting up storage...\")\n",
    "    print(\"   ‚ö†Ô∏è  Google Drive mounting not supported in VS Code + Colab extension\")\n",
    "    print(\"   ‚úì  Using /content/ directory (persists during session)\")\n",
    "    print(\"   üí° Download results at end of session\")\n",
    "    \n",
    "    # Create checkpoint and output directories in /content/\n",
    "    !mkdir -p /content/sgm_checkpoints\n",
    "    !mkdir -p /content/sgm_outputs\n",
    "    \n",
    "    # Clone/sync repo to Colab runtime\n",
    "    # NOTE: When using VS Code + Colab extension, your LOCAL files are NOT automatically\n",
    "    # available to the REMOTE Colab runtime. We need to clone the repo.\n",
    "    repo_url = \"https://github.com/ChuloIva/align_prompts\"  # UPDATE THIS\n",
    "    \n",
    "    if not Path('/content/align_prompts').exists():\n",
    "        print(f\"\\n4. Cloning repository to Colab runtime...\")\n",
    "        print(f\"   Repo: {repo_url}\")\n",
    "        !git clone {repo_url} /content/align_prompts\n",
    "    else:\n",
    "        print(f\"\\n4. Repository already exists. Pulling latest changes...\")\n",
    "        !cd /content/align_prompts && git pull\n",
    "    \n",
    "    # Add repo to Python path\n",
    "    sys.path.insert(0, '/content/align_prompts')\n",
    "    print(f\"   Added to sys.path: /content/align_prompts\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Colab runtime ready!\")\n",
    "    print(\"\\nüí° Important Notes:\")\n",
    "    print(\"   ‚Ä¢ Your notebook file is local, but code runs on Colab GPU\")\n",
    "    print(\"   ‚Ä¢ Files saved to /content/ persist during your session\")\n",
    "    print(\"   ‚Ä¢ Download results before disconnecting (session timeout ~12hrs)\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n=== Local Environment ===\")\n",
    "    print(\"Make sure you have:\")\n",
    "    print(\"  - vllm installed: pip install vllm\")\n",
    "    print(\"  - networkx installed: pip install networkx\")\n",
    "    print(\"  - tqdm, matplotlib, seaborn: pip install tqdm matplotlib seaborn\")\n",
    "    print(\"  - vLLM server running on localhost:8000\")\n",
    "    print(\"\\nOr install Colab extension for free GPU: https://marketplace.visualstudio.com/items?itemName=Google.colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Configuration\n",
    "\n",
    "Set model, paths, and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    'model': 'google/gemma-3-4b-it',  # Model to use\n",
    "    'vllm_base_url': 'http://localhost:8000/v1',  # vLLM server URL\n",
    "    \n",
    "    # Graph generation settings\n",
    "    'max_hops': 3,  # BFS depth (3 = ~15k edges)\n",
    "    'associations_per_word': 5,  # Associations per word\n",
    "    \n",
    "    # Checkpoint settings (using /content/ instead of Google Drive)\n",
    "    'checkpoint_dir': '/content/sgm_checkpoints' if IN_COLAB else './data/sgm/checkpoints',\n",
    "    'output_dir': '/content/sgm_outputs' if IN_COLAB else './data/sgm/graphs',\n",
    "    \n",
    "    # Optimization settings\n",
    "    'temperature_associations': 0.7,  # Temperature for Phase 1\n",
    "    'temperature_scoring': 0.0,  # Temperature for Phase 2 (deterministic)\n",
    "    'batch_size': 32,  # Concurrent requests\n",
    "    \n",
    "    # Resume settings\n",
    "    'resume': True  # Resume from checkpoint if available\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2.5: Hugging Face Authentication\n",
    "\n",
    "Gemma models are gated - you need to authenticate with Hugging Face to download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if IN_COLAB:\n    print(\"üîê Hugging Face Authentication Required\")\n    print(\"=\" * 50)\n    print(f\"\\nModel '{CONFIG['model']}' is a gated model.\")\n    print(\"\\nüìã Steps to get access:\")\n    print(\"   1. Go to: https://huggingface.co/google/gemma-3-4b-it\")\n    print(\"   2. Click 'Agree and access repository'\")\n    print(\"   3. Get your token: https://huggingface.co/settings/tokens\")\n    print(\"   4. Create a token with 'read' permissions\")\n    print(\"\\n\")\n    \n    # Try to use huggingface-cli login\n    try:\n        from huggingface_hub import login\n        import getpass\n        \n        # Check if already logged in\n        try:\n            from huggingface_hub import HfFolder\n            token = HfFolder.get_token()\n            if token:\n                print(\"‚úÖ Already logged in to Hugging Face!\")\n            else:\n                raise Exception(\"Not logged in\")\n        except:\n            print(\"Please paste your Hugging Face token below:\")\n            print(\"(Token will be hidden while typing)\")\n            hf_token = getpass.getpass(\"HF Token: \")\n            \n            # Login with token\n            login(token=hf_token, add_to_git_credential=False)\n            print(\"\\n‚úÖ Successfully authenticated with Hugging Face!\")\n    \n    except ImportError:\n        print(\"‚ö†Ô∏è  huggingface_hub not installed. Installing...\")\n        !pip install -q huggingface_hub\n        print(\"‚úÖ Installed. Please re-run this cell to authenticate.\")\n        \nelse:\n    print(\"Local environment - make sure you're logged in to Hugging Face:\")\n    print(\"  huggingface-cli login\")\n    print(\"\\nOr set HF_TOKEN environment variable:\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Starting vLLM server in background...\")\n",
    "    print(f\"Model: {CONFIG['model']}\")\n",
    "    print(\"\\n‚è±Ô∏è  Note: First run may take 5-10 minutes to download the model (~8GB)\")\n",
    "    print(\"   Subsequent runs will be much faster (model is cached)\\n\")\n",
    "    \n",
    "    # Kill any existing vLLM servers first\n",
    "    print(\"1. Cleaning up any existing vLLM processes...\")\n",
    "    !pkill -f \"vllm.entrypoints.openai.api_server\" 2>/dev/null || true\n",
    "    !sleep 2\n",
    "    \n",
    "    # Start vLLM server in background with more verbose logging\n",
    "    print(\"2. Starting vLLM server...\")\n",
    "    vllm_cmd = f\"\"\"\n",
    "    nohup python -m vllm.entrypoints.openai.api_server \\\n",
    "        --model {CONFIG['model']} \\\n",
    "        --gpu-memory-utilization 0.9 \\\n",
    "        --max-model-len 2048 \\\n",
    "        --port 8000 \\\n",
    "        --trust-remote-code \\\n",
    "        > /tmp/vllm_server.log 2>&1 &\n",
    "    \"\"\"\n",
    "    \n",
    "    !{vllm_cmd}\n",
    "    \n",
    "    # Wait for server to be ready with better monitoring\n",
    "    import time\n",
    "    import requests\n",
    "    \n",
    "    print(\"3. Waiting for vLLM server to initialize...\")\n",
    "    print(\"   (Checking server health every 5 seconds)\\n\")\n",
    "    \n",
    "    max_wait_time = 600  # 10 minutes\n",
    "    check_interval = 5   # 5 seconds\n",
    "    max_iterations = max_wait_time // check_interval\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        try:\n",
    "            response = requests.get('http://localhost:8000/health', timeout=2)\n",
    "            if response.status_code == 200:\n",
    "                elapsed = i * check_interval\n",
    "                print(f\"\\n‚úÖ vLLM server is ready! (took {elapsed}s)\")\n",
    "                \n",
    "                # Test the server with a simple request\n",
    "                print(\"\\n4. Testing server with sample request...\")\n",
    "                try:\n",
    "                    test_response = requests.post(\n",
    "                        'http://localhost:8000/v1/completions',\n",
    "                        json={'model': CONFIG['model'], 'prompt': 'Hello', 'max_tokens': 5},\n",
    "                        timeout=10\n",
    "                    )\n",
    "                    if test_response.status_code == 200:\n",
    "                        print(\"‚úÖ Server test successful!\")\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è  Server responded but with status {test_response.status_code}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è  Server test failed: {e}\")\n",
    "                break\n",
    "        except requests.exceptions.RequestException:\n",
    "            pass\n",
    "        \n",
    "        # Show progress every 20 seconds\n",
    "        if i > 0 and i % 4 == 0:\n",
    "            elapsed = i * check_interval\n",
    "            print(f\"   Still initializing... ({elapsed}s elapsed)\")\n",
    "            \n",
    "            # Show last few lines of log for progress\n",
    "            print(\"   Latest log output:\")\n",
    "            !tail -n 3 /tmp/vllm_server.log 2>/dev/null | sed 's/^/     /'\n",
    "            print()\n",
    "        \n",
    "        time.sleep(check_interval)\n",
    "    else:\n",
    "        elapsed = max_iterations * check_interval\n",
    "        print(f\"\\n‚ö†Ô∏è  Server didn't respond after {elapsed}s\")\n",
    "        print(\"\\nüìã Full server log:\")\n",
    "        !cat /tmp/vllm_server.log\n",
    "        print(\"\\nüí° Troubleshooting:\")\n",
    "        print(\"   ‚Ä¢ Check if vLLM process is running: !ps aux | grep vllm\")\n",
    "        print(\"   ‚Ä¢ Check GPU memory: !nvidia-smi\")\n",
    "        print(\"   ‚Ä¢ Make sure you're authenticated with Hugging Face (run previous cell)\")\n",
    "        print(\"   ‚Ä¢ Try restarting the Colab runtime\")\n",
    "        print(\"   ‚Ä¢ The model might be too large for the GPU\")\n",
    "        \n",
    "else:\n",
    "    print(\"Local environment - assuming vLLM server is already running.\")\n",
    "    print(f\"Make sure vLLM is serving {CONFIG['model']} on {CONFIG['vllm_base_url']}\")\n",
    "    print(\"\\nTo start vLLM locally, run:\")\n",
    "    print(f\"  python -m vllm.entrypoints.openai.api_server --model {CONFIG['model']} --port 8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Starting vLLM server in background...\")\n",
    "    print(f\"Model: {CONFIG['model']}\")\n",
    "    print(\"\\n‚è±Ô∏è  Note: First run may take 5-10 minutes to download the model (~8GB)\")\n",
    "    print(\"   Subsequent runs will be much faster (model is cached)\\n\")\n",
    "    \n",
    "    # Kill any existing vLLM servers first\n",
    "    print(\"1. Cleaning up any existing vLLM processes...\")\n",
    "    !pkill -f \"vllm.entrypoints.openai.api_server\" 2>/dev/null || true\n",
    "    !sleep 2\n",
    "    \n",
    "    # Start vLLM server in background with more verbose logging\n",
    "    print(\"2. Starting vLLM server...\")\n",
    "    vllm_cmd = f\"\"\"\n",
    "    nohup python -m vllm.entrypoints.openai.api_server \\\n",
    "        --model {CONFIG['model']} \\\n",
    "        --gpu-memory-utilization 0.9 \\\n",
    "        --max-model-len 2048 \\\n",
    "        --port 8000 \\\n",
    "        --trust-remote-code \\\n",
    "        > /tmp/vllm_server.log 2>&1 &\n",
    "    \"\"\"\n",
    "    \n",
    "    !{vllm_cmd}\n",
    "    \n",
    "    # Wait for server to be ready with better monitoring\n",
    "    import time\n",
    "    import requests\n",
    "    \n",
    "    print(\"3. Waiting for vLLM server to initialize...\")\n",
    "    print(\"   (Checking server health every 5 seconds)\\n\")\n",
    "    \n",
    "    max_wait_time = 600  # 10 minutes\n",
    "    check_interval = 5   # 5 seconds\n",
    "    max_iterations = max_wait_time // check_interval\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        try:\n",
    "            response = requests.get('http://localhost:8000/health', timeout=2)\n",
    "            if response.status_code == 200:\n",
    "                elapsed = i * check_interval\n",
    "                print(f\"\\n‚úÖ vLLM server is ready! (took {elapsed}s)\")\n",
    "                \n",
    "                # Test the server with a simple request\n",
    "                print(\"\\n4. Testing server with sample request...\")\n",
    "                try:\n",
    "                    test_response = requests.post(\n",
    "                        'http://localhost:8000/v1/completions',\n",
    "                        json={'model': CONFIG['model'], 'prompt': 'Hello', 'max_tokens': 5},\n",
    "                        timeout=10\n",
    "                    )\n",
    "                    if test_response.status_code == 200:\n",
    "                        print(\"‚úÖ Server test successful!\")\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è  Server responded but with status {test_response.status_code}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è  Server test failed: {e}\")\n",
    "                break\n",
    "        except requests.exceptions.RequestException:\n",
    "            pass\n",
    "        \n",
    "        # Show progress every 20 seconds\n",
    "        if i > 0 and i % 4 == 0:\n",
    "            elapsed = i * check_interval\n",
    "            print(f\"   Still initializing... ({elapsed}s elapsed)\")\n",
    "            \n",
    "            # Show last few lines of log for progress\n",
    "            print(\"   Latest log output:\")\n",
    "            !tail -n 3 /tmp/vllm_server.log 2>/dev/null | sed 's/^/     /'\n",
    "            print()\n",
    "        \n",
    "        time.sleep(check_interval)\n",
    "    else:\n",
    "        elapsed = max_iterations * check_interval\n",
    "        print(f\"\\n‚ö†Ô∏è  Server didn't respond after {elapsed}s\")\n",
    "        print(\"\\nüìã Full server log:\")\n",
    "        !cat /tmp/vllm_server.log\n",
    "        print(\"\\nüí° Troubleshooting:\")\n",
    "        print(\"   ‚Ä¢ Check if vLLM process is running: !ps aux | grep vllm\")\n",
    "        print(\"   ‚Ä¢ Check GPU memory: !nvidia-smi\")\n",
    "        print(\"   ‚Ä¢ Make sure you're authenticated with Hugging Face (run previous cell)\")\n",
    "        print(\"   ‚Ä¢ Try restarting the Colab runtime\")\n",
    "        print(\"   ‚Ä¢ The model might be too large for the GPU\")\n",
    "        \n",
    "else:\n",
    "    print(\"Local environment - assuming vLLM server is already running.\")\n",
    "    print(f\"Make sure vLLM is serving {CONFIG['model']} on {CONFIG['vllm_base_url']}\")\n",
    "    print(\"\\nTo start vLLM locally, run:\")\n",
    "    print(f\"  python -m vllm.entrypoints.openai.api_server --model {CONFIG['model']} --port 8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: Check vLLM Server Status\n",
    "\n",
    "Run this cell if the server isn't starting or you want to see what's happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper cell to check vLLM server status\n",
    "if IN_COLAB:\n",
    "    print(\"üîç vLLM Server Diagnostics\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Check if process is running\n",
    "    print(\"\\n1. Checking if vLLM process is running:\")\n",
    "    !ps aux | grep -E \"[v]llm.entrypoints\" || echo \"   ‚ùå No vLLM process found\"\n",
    "    \n",
    "    # Check port\n",
    "    print(\"\\n2. Checking if port 8000 is in use:\")\n",
    "    !lsof -i :8000 || echo \"   ‚ùå Port 8000 not in use\"\n",
    "    \n",
    "    # Check server health endpoint\n",
    "    print(\"\\n3. Testing health endpoint:\")\n",
    "    import requests\n",
    "    try:\n",
    "        response = requests.get('http://localhost:8000/health', timeout=2)\n",
    "        print(f\"   ‚úÖ Server responding! Status: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Server not responding: {e}\")\n",
    "    \n",
    "    # Show recent logs\n",
    "    print(\"\\n4. Recent server logs (last 20 lines):\")\n",
    "    print(\"-\" * 50)\n",
    "    !tail -n 20 /tmp/vllm_server.log 2>/dev/null || echo \"   ‚ùå No log file found\"\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # GPU status\n",
    "    print(\"\\n5. GPU Memory Status:\")\n",
    "    !nvidia-smi --query-gpu=memory.used,memory.total --format=csv\n",
    "    \n",
    "    print(\"\\nüí° To see full logs, run: !cat /tmp/vllm_server.log\")\n",
    "    print(\"üí° To restart server, re-run the 'Start vLLM Server' cell above\")\n",
    "    \n",
    "else:\n",
    "    print(\"Local environment - check your vLLM server manually\")\n",
    "    print(\"  ps aux | grep vllm\")\n",
    "    print(\"  curl http://localhost:8000/health\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Starting vLLM server in background...\")\n",
    "    print(f\"Model: {CONFIG['model']}\")\n",
    "    print(\"\\nThis may take 2-3 minutes to download and load the model.\")\n",
    "    \n",
    "    # Start vLLM server in background\n",
    "    vllm_cmd = f\"\"\"\n",
    "    nohup python -m vllm.entrypoints.openai.api_server \\\n",
    "        --model {CONFIG['model']} \\\n",
    "        --gpu-memory-utilization 0.9 \\\n",
    "        --max-model-len 2048 \\\n",
    "        --port 8000 \\\n",
    "        > /tmp/vllm_server.log 2>&1 &\n",
    "    \"\"\"\n",
    "    \n",
    "    !{vllm_cmd}\n",
    "    \n",
    "    # Wait for server to be ready\n",
    "    import time\n",
    "    import requests\n",
    "    \n",
    "    print(\"\\nWaiting for vLLM server to start...\")\n",
    "    for i in range(60):  # Wait up to 60 seconds\n",
    "        try:\n",
    "            response = requests.get('http://localhost:8000/health')\n",
    "            if response.status_code == 200:\n",
    "                print(\"\\n‚úÖ vLLM server is ready!\")\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        time.sleep(2)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"  Still waiting... ({i*2}s)\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Server didn't respond in time. Check logs: !tail /tmp/vllm_server.log\")\n",
    "        \n",
    "else:\n",
    "    print(\"Local environment - assuming vLLM server is already running.\")\n",
    "    print(f\"Make sure vLLM is serving {CONFIG['model']} on {CONFIG['vllm_base_url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Initialize Components\n",
    "\n",
    "Create engine, checkpoint manager, and check for existing checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from align_test.core.vllm_client import VLLMClient\n",
    "from align_test.sgm.inference.batch_inference import SGMInferenceEngine\n",
    "from align_test.sgm.storage.checkpoint_manager import CheckpointManager\n",
    "from align_test.sgm.models.seed_domains import get_all_seeds, get_domain_names\n",
    "\n",
    "# Initialize vLLM client\n",
    "print(\"Initializing components...\\n\")\n",
    "\n",
    "vllm_client = VLLMClient(\n",
    "    base_url=CONFIG['vllm_base_url'],\n",
    "    model=CONFIG['model']\n",
    ")\n",
    "\n",
    "print(f\"‚úì VLLMClient: {vllm_client}\")\n",
    "\n",
    "# Initialize inference engine\n",
    "inference_engine = SGMInferenceEngine(\n",
    "    client=vllm_client,\n",
    "    temperature=CONFIG['temperature_associations'],\n",
    "    batch_size=CONFIG['batch_size']\n",
    ")\n",
    "\n",
    "print(f\"‚úì SGMInferenceEngine: {inference_engine}\")\n",
    "\n",
    "# Initialize checkpoint manager\n",
    "checkpoint_manager = CheckpointManager(\n",
    "    checkpoint_dir=CONFIG['checkpoint_dir'],\n",
    "    config=CONFIG\n",
    ")\n",
    "\n",
    "print(f\"‚úì CheckpointManager: {checkpoint_manager.checkpoint_dir}\")\n",
    "\n",
    "# Check for existing checkpoints\n",
    "resume_info = checkpoint_manager.get_resume_info()\n",
    "\n",
    "if resume_info:\n",
    "    print(f\"\\nüìÅ Found checkpoint: Phase {resume_info['phase']}, Iteration {resume_info['iteration']}\")\n",
    "    print(f\"   Timestamp: {resume_info['timestamp']}\")\n",
    "    print(f\"   Can resume from: {resume_info['checkpoint_file']}\")\n",
    "else:\n",
    "    print(\"\\nüìÅ No existing checkpoints found - starting fresh\")\n",
    "\n",
    "# Display seed information\n",
    "seeds = get_all_seeds()\n",
    "domains = get_domain_names()\n",
    "\n",
    "print(f\"\\nüå± Seeds: {len(seeds)} concepts across {len(domains)} domains\")\n",
    "print(f\"   Domains: {', '.join(domains)}\")\n",
    "print(f\"   Sample seeds: {', '.join(seeds[:5])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Phase 1 - Seed & Crawl\n",
    "\n",
    "Generate association graph via BFS expansion. This will take 30-60 minutes on T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from align_test.sgm.core.graph_builder import GraphBuilder\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: SEED & CRAWL - BFS Graph Generation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize graph builder\n",
    "graph_builder = GraphBuilder(\n",
    "    engine=inference_engine,\n",
    "    checkpoint_manager=checkpoint_manager,\n",
    "    checkpoint_interval=500\n",
    ")\n",
    "\n",
    "# Build graph\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "raw_graph = graph_builder.build_graph(\n",
    "    max_hops=CONFIG['max_hops'],\n",
    "    associations_per_word=CONFIG['associations_per_word'],\n",
    "    resume=CONFIG['resume']\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Phase 1 completed in {elapsed_time/60:.1f} minutes\")\n",
    "print(f\"\\nüìä Final Graph:\")\n",
    "print(f\"   Nodes: {raw_graph.number_of_nodes():,}\")\n",
    "print(f\"   Edges: {raw_graph.number_of_edges():,}\")\n",
    "\n",
    "# Get statistics\n",
    "stats = graph_builder.get_statistics()\n",
    "print(f\"\\nüìà Statistics:\")\n",
    "print(f\"   Visited nodes: {stats['num_visited']:,}\")\n",
    "print(f\"   Avg out-degree: {stats['avg_out_degree']:.2f}\")\n",
    "print(f\"   Max out-degree: {stats['max_out_degree']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Phase 1 Results Preview\n",
    "\n",
    "Visualize sample associations and graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 1: RESULTS PREVIEW\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show sample paths\n",
    "print(\"\\nüîç Sample Association Paths:\")\n",
    "graph_builder.preview_sample_paths(n=5)\n",
    "\n",
    "# Analyze hop distribution\n",
    "hop_counts = {}\n",
    "for _, _, data in raw_graph.edges(data=True):\n",
    "    hop = data.get('hop', 0)\n",
    "    hop_counts[hop] = hop_counts.get(hop, 0) + 1\n",
    "\n",
    "print(\"\\nüìä Edge Distribution by Hop:\")\n",
    "for hop in sorted(hop_counts.keys()):\n",
    "    count = hop_counts[hop]\n",
    "    print(f\"   Hop {hop}: {count:,} edges ({count/raw_graph.number_of_edges()*100:.1f}%)\")\n",
    "\n",
    "# Visualize hop distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(hop_counts.keys(), hop_counts.values())\n",
    "plt.xlabel('Hop')\n",
    "plt.ylabel('Number of Edges')\n",
    "plt.title('Edge Distribution by BFS Hop')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Show top nodes by degree\n",
    "degree_dict = dict(raw_graph.in_degree())\n",
    "top_nodes = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"\\nüéØ Top 10 Nodes by In-Degree (most associated with):\")\n",
    "for i, (node, degree) in enumerate(top_nodes, 1):\n",
    "    print(f\"   {i:2d}. {node:20s} (degree: {degree})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Phase 2 - Logprob Scoring\n",
    "\n",
    "Score edge weights using logprob extraction. This will take 10-20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from align_test.sgm.core.logprob_scorer import LogprobScorer\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2: LOGPROB SCORING - Edge Weight Assignment\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Update engine temperature for deterministic scoring\n",
    "inference_engine.temperature = CONFIG['temperature_scoring']\n",
    "\n",
    "# Initialize scorer\n",
    "logprob_scorer = LogprobScorer(\n",
    "    engine=inference_engine,\n",
    "    checkpoint_manager=checkpoint_manager,\n",
    "    checkpoint_interval=2000\n",
    ")\n",
    "\n",
    "# Score all edges\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "weighted_graph = logprob_scorer.score_all_edges(\n",
    "    graph=raw_graph,\n",
    "    resume=CONFIG['resume'],\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Phase 2 completed in {elapsed_time/60:.1f} minutes\")\n",
    "\n",
    "# Get weight statistics\n",
    "weight_stats = logprob_scorer.get_weight_statistics(weighted_graph)\n",
    "\n",
    "print(f\"\\nüìä Weight Statistics:\")\n",
    "print(f\"   Mean: {weight_stats['mean_weight']:.4f}\")\n",
    "print(f\"   Median: {weight_stats['median_weight']:.4f}\")\n",
    "print(f\"   Min: {weight_stats['min_weight']:.4f}\")\n",
    "print(f\"   Max: {weight_stats['max_weight']:.4f}\")\n",
    "print(f\"   Std: {weight_stats['std_weight']:.4f}\")\n",
    "print(f\"   Scored edges: {weight_stats['num_scored']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Phase 2 Results Preview\n",
    "\n",
    "Analyze strongest and weakest associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PHASE 2: RESULTS PREVIEW\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Show top edges (strongest associations)\n",
    "top_edges = logprob_scorer.get_top_edges(weighted_graph, n=10, sort_by='weight')\n",
    "\n",
    "print(\"\\nüí™ Top 10 Strongest Associations (by weight):\")\n",
    "for i, (u, v, w) in enumerate(top_edges, 1):\n",
    "    print(f\"   {i:2d}. {u:15s} ‚Üí {v:15s} (weight: {w:.4f})\")\n",
    "\n",
    "# Show bottom edges (weakest associations)\n",
    "bottom_edges = logprob_scorer.get_bottom_edges(weighted_graph, n=10, sort_by='weight')\n",
    "\n",
    "print(\"\\nüîª Top 10 Weakest Associations (by weight):\")\n",
    "for i, (u, v, w) in enumerate(bottom_edges, 1):\n",
    "    print(f\"   {i:2d}. {u:15s} ‚Üí {v:15s} (weight: {w:.4f})\")\n",
    "\n",
    "# Visualize weight distribution\n",
    "print(\"\\nüìä Weight Distribution:\")\n",
    "logprob_scorer.visualize_weight_distribution(weighted_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Phase 3 - Topology Analysis\n",
    "\n",
    "Analyze graph topology to find hubs, convergence patterns, islands, and asymmetries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from align_test.sgm.core.topology_analyzer import TopologyAnalyzer\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PHASE 3: TOPOLOGY ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = TopologyAnalyzer(weighted_graph)\n",
    "\n",
    "# Run all analyses\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "results = analyzer.analyze_all()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Phase 3 completed in {elapsed_time:.1f} seconds\")\n",
    "\n",
    "# Print summary\n",
    "analyzer.print_summary(results)\n",
    "\n",
    "# Export results to JSON\n",
    "output_path = Path(CONFIG['output_dir']) / 'topology_metrics.json'\n",
    "analyzer.export_results(results, str(output_path))\n",
    "\n",
    "# Save final graph\n",
    "print(\"\\nüíæ Saving final graph...\")\n",
    "checkpoint_manager.save_graph(\n",
    "    graph=weighted_graph,\n",
    "    filename='semantic_graph_final',\n",
    "    include_metadata=True\n",
    ")\n",
    "print(f\"   Graph saved to: {CONFIG['output_dir']}/semantic_graph_final.gpickle\")\n",
    "print(f\"   Edge list (CSV): {CONFIG['output_dir']}/semantic_graph_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Results Summary & Export\n",
    "\n",
    "Final summary and file access instructions (results are saved to Google Drive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"üéâ SEMANTIC GRAVITY MAPPING - COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Summary of findings\n",
    "print(\"\\nüìù Key Findings:\")\n",
    "print(\"\\n1. Semantic Attractors (Hubs):\")\n",
    "top_hubs = results['hubs'][:5]\n",
    "for hub in top_hubs:\n",
    "    print(f\"   ‚Ä¢ {hub['word']} (PageRank: {hub['pagerank']:.4f})\")\n",
    "\n",
    "print(\"\\n2. Convergence Analysis:\")\n",
    "conv = results['convergence']\n",
    "print(f\"   ‚Ä¢ Overall avg hops to hubs: {conv['overall_avg_hops']:.2f}\")\n",
    "fastest_domain = min(conv['by_domain'].items(), key=lambda x: x[1]['avg_hops'])\n",
    "print(f\"   ‚Ä¢ Fastest converging domain: {fastest_domain[0]} ({fastest_domain[1]['avg_hops']:.2f} hops)\")\n",
    "\n",
    "print(\"\\n3. Isolated Domains (Islands):\")\n",
    "if results['islands']:\n",
    "    for island in results['islands'][:3]:\n",
    "        print(f\"   ‚Ä¢ Size {island['size']}: {', '.join(island['words'][:3])}...\")\n",
    "else:\n",
    "    print(\"   ‚Ä¢ No isolated clusters found\")\n",
    "\n",
    "print(\"\\n4. Asymmetric Associations (Narrative Bias):\")\n",
    "for pair in results['asymmetry'][:3]:\n",
    "    print(f\"   ‚Ä¢ {pair['source']} ‚Üí {pair['target']}: {pair['asymmetry']:.3f}\")\n",
    "\n",
    "# Output files\n",
    "print(\"\\n\\nüì¶ Output Files:\")\n",
    "print(f\"   ‚Ä¢ Graph (pickle): {CONFIG['output_dir']}/semantic_graph_final.gpickle\")\n",
    "print(f\"   ‚Ä¢ Edge list (CSV): {CONFIG['output_dir']}/semantic_graph_final.csv\")\n",
    "print(f\"   ‚Ä¢ Metrics (JSON): {CONFIG['output_dir']}/topology_metrics.json\")\n",
    "print(f\"   ‚Ä¢ Checkpoints: {CONFIG['checkpoint_dir']}/\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\\nüí° How to Download Your Results:\")\n",
    "    print(\"   Files are saved to /content/sgm_outputs/ on the Colab runtime\")\n",
    "    print(\"\\n   Option 1 - Direct Download (Traditional Colab):\")\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        download = input(\"\\n   üì• Download results now? (y/n): \")\n",
    "        if download.lower() == 'y':\n",
    "            print(\"   Downloading...\")\n",
    "            files.download(f\"{CONFIG['output_dir']}/semantic_graph_final.csv\")\n",
    "            files.download(f\"{CONFIG['output_dir']}/topology_metrics.json\")\n",
    "            files.download(f\"{CONFIG['output_dir']}/semantic_graph_final.gpickle\")\n",
    "            print(\"   ‚úÖ Download complete!\")\n",
    "    except:\n",
    "        print(\"   ‚ö†Ô∏è  Direct download not available in VS Code + Colab extension\")\n",
    "        print(\"\\n   Option 2 - Upload to GitHub:\")\n",
    "        print(\"   Run these commands to push results to your repo:\")\n",
    "        print(f\"   !cd /content/align_prompts && mkdir -p data/results\")\n",
    "        print(f\"   !cp {CONFIG['output_dir']}/* /content/align_prompts/data/results/\")\n",
    "        print(f\"   !cd /content/align_prompts && git add data/results && git commit -m 'Add SGM results' && git push\")\n",
    "        print(\"\\n   Option 3 - Manual Copy (VS Code):\")\n",
    "        print(\"   1. Use VS Code file browser to navigate to /content/sgm_outputs/\")\n",
    "        print(\"   2. Right-click files ‚Üí Download\")\n",
    "        print(\"   3. Or upload to a cloud storage service\")\n",
    "    \n",
    "    print(\"\\n   ‚ö†Ô∏è  Remember: /content/ is temporary - download before session ends!\")\n",
    "    \n",
    "    print(\"\\n   Next Steps:\")\n",
    "    print(\"   1. Download the CSV and JSON files\")\n",
    "    print(\"   2. Import CSV to Gephi for network visualization\")\n",
    "    print(\"   3. Analyze topology_metrics.json for detailed insights\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Thank you for using Semantic Gravity Mapping!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper: Download Results (VS Code + Colab)\n",
    "\n",
    "If you're using VS Code + Colab extension, run this cell to package and download your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create a zip file of all results for easy download\n",
    "if IN_COLAB:\n",
    "    import shutil\n",
    "    \n",
    "    print(\"üì¶ Packaging results for download...\")\n",
    "    \n",
    "    # Create zip archive\n",
    "    output_zip = '/content/sgm_results'\n",
    "    shutil.make_archive(output_zip, 'zip', CONFIG['output_dir'])\n",
    "    \n",
    "    print(f\"‚úÖ Created: {output_zip}.zip\")\n",
    "    print(f\"   Size: {Path(f'{output_zip}.zip').stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # Try to download (works in traditional Colab, not VS Code)\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        print(\"\\nüì• Downloading zip file...\")\n",
    "        files.download(f'{output_zip}.zip')\n",
    "        print(\"‚úÖ Download started!\")\n",
    "    except:\n",
    "        print(\"\\n‚ö†Ô∏è  Automatic download not available in VS Code + Colab\")\n",
    "        print(f\"\\nüí° Alternative: Copy {output_zip}.zip to your GitHub repo:\")\n",
    "        print(f\"   !cp {output_zip}.zip /content/align_prompts/sgm_results.zip\")\n",
    "        print(f\"   !cd /content/align_prompts && git add sgm_results.zip && git commit -m 'Add results' && git push\")\n",
    "        print(\"\\nThen download from GitHub, or right-click the file in VS Code file browser.\")\n",
    "else:\n",
    "    print(\"Local environment - results already saved locally to:\")\n",
    "    print(f\"  {CONFIG['output_dir']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}